# Pipeline Offline PDF â†’ (empresa, funcionÃ¡rio)

Pipeline em **Python 3.11** para extrair informaÃ§Ãµes de **empresa** e **funcionÃ¡rio** de documentos PDF usando processamento local e modelos de linguagem.

## ğŸ“‹ VisÃ£o Geral

Este projeto implementa um pipeline de extraÃ§Ã£o de informaÃ§Ãµes estruturadas a partir de PDFs, combinando:

- **Docling**: ConversÃ£o de PDF para estrutura JSON
- **spaCy**: Reconhecimento de entidades nomeadas (NER) e heurÃ­sticas
- **llama.cpp**: DecisÃ£o final usando modelos de linguagem locais (GGUF)

O pipeline processa documentos **localmente** (sem enviar dados para servidores externos) e faz **downloads automÃ¡ticos** de modelos e dependÃªncias quando necessÃ¡rio.

## ğŸ—ï¸ Arquitetura

O pipeline segue uma arquitetura modular em etapas:

1. **ExtraÃ§Ã£o (extract_json)**: Converte PDF para JSON estruturado usando Docling
2. **Blocos (blocks)**: Processa e normaliza os blocos de texto extraÃ­dos
3. **Candidatos (candidates)**: Gera candidatos para empresa e funcionÃ¡rio usando spaCy NER
4. **PontuaÃ§Ã£o (scoring)**: Classifica e ranqueia os candidatos com base em heurÃ­sticas
5. **DecisÃ£o LLM (decision_llm)**: Usa modelo de linguagem local para decisÃ£o final
6. **ConfianÃ§a (confidence)**: Calcula mÃ©tricas de confianÃ§a para os resultados

## ğŸš€ Requisitos

- **Sistema Operacional**: Windows 10/11
- **Python**: 3.11.x (recomendado)
- **ConexÃ£o com internet**: NecessÃ¡ria para downloads automÃ¡ticos de dependÃªncias e modelos
- **Modelo GGUF**: Modelo de linguagem local em formato GGUF (ex.: `models/model.gguf`) - opcional

## ğŸ“¦ InstalaÃ§Ã£o

### 1. Criar ambiente virtual

```bash
py -3.11 -m venv .venv
.\.venv\Scripts\activate
```

### 2. Instalar dependÃªncias

O projeto faz **downloads automÃ¡ticos** de todas as dependÃªncias necessÃ¡rias:

```bash
pip install -r requirements.txt
```

### 3. Instalar modelo spaCy

O modelo spaCy `pt_core_news_lg` serÃ¡ baixado automaticamente na primeira execuÃ§Ã£o. Para instalar manualmente:

```bash
python -m spacy download pt_core_news_lg
```

### 4. Configurar modelo GGUF (opcional)

Coloque seu modelo GGUF em `models/` (ex.: `models/model.gguf`).

**Modelo recomendado: Qwen**

Para usar o modelo Qwen, baixe uma versÃ£o GGUF do repositÃ³rio oficial:

- [Qwen2.5 GGUF Models](https://huggingface.co/Qwen/Qwen2.5-7B-Instruct-GGUF)
  Para testes iniciais, usamos o modelo Qween 2.5 7b q4 k_m
  https://huggingface.co/Qwen/Qwen2.5-7B-Instruct-GGUF/blob/main/qwen2.5-7b-instruct-q4_k_m-00001-of-00002.gguf
  https://huggingface.co/Qwen/Qwen2.5-7B-Instruct-GGUF/blob/main/qwen2.5-7b-instruct-q4_k_m-00002-of-00002.gguf

ApÃ³s o download, coloque o arquivo `.gguf` na pasta `models/` e use com a flag `--chat-format qwen`:

```bash
python main.py --pdf documento.pdf --out output\result.json --model models\qwen2.5-7b-instruct-q4_k_m-00001-of-00002.gguff --chat-format qwen
```

## ğŸ’» Uso

### Uso bÃ¡sico

```bash
python main.py --pdf examples\sample.pdf --out output\result.json --model models\model.gguf
```

### OpÃ§Ãµes disponÃ­veis

- `--pdf`: Caminho para o PDF de entrada (obrigatÃ³rio)
- `--out`: Caminho para o arquivo JSON de saÃ­da (obrigatÃ³rio)
- `--model`: Caminho para o modelo GGUF (opcional)
- `--chat-format`: Formato de chat do llama.cpp (ex.: `qwen`) - Ãºtil quando o modelo precisa de um template explÃ­cito
- `--offline`: Desabilita acesso Ã  rede (sem downloads de modelos). Se os modelos necessÃ¡rios estiverem faltando, a saÃ­da serÃ¡ `INDEFINIDO`
- `--debug`: Inclui detalhes de debug no JSON de saÃ­da

### Exemplo com modo offline

```bash
python main.py --pdf funcionario.pdf --out output\result.json --offline
```

## ğŸ“¤ Formato de SaÃ­da

O pipeline gera um arquivo JSON com a seguinte estrutura:

```json
{
  "funcionario": "JoÃ£o Silva",
  "empresa": "Empresa XYZ Ltda",
  "confidence": {
    "funcionario": 0.85,
    "empresa": 0.92
  },
  "debug": {
    "extraction_quality": "ok",
    "blocks_count": 45,
    "candidates_count": {
      "funcionarios": 12,
      "empresas": 8
    },
    "top_ranked": {
      "funcionario": [
        { "text": "JoÃ£o Silva", "score": 0.95 },
        { "text": "J. Silva", "score": 0.7 }
      ],
      "empresa": [{ "text": "Empresa XYZ Ltda", "score": 0.98 }]
    },
    "llm_used": true
  }
}
```

### Casos de erro

Em caso de falha na extraÃ§Ã£o ou erro de processamento, a saÃ­da serÃ¡:

```json
{
  "funcionario": "INDEFINIDO",
  "empresa": "INDEFINIDO",
  "confidence": {
    "funcionario": 0.0,
    "empresa": 0.0
  },
  "debug": {
    "extraction_quality": "weak",
    "error": "extract_failed:ExceptionName"
  }
}
```

## ğŸ”’ Privacidade e SeguranÃ§a

- **Processamento local**: O pipeline nÃ£o envia seu PDF nem resultados para nenhum servidor. Todo o processamento acontece na sua mÃ¡quina
- **Downloads automÃ¡ticos**: Por padrÃ£o, o projeto faz downloads automÃ¡ticos de modelos e dependÃªncias quando necessÃ¡rio (Docling, spaCy, etc.)
- **Modo offline**: Use a flag `--offline` para desabilitar downloads durante a execuÃ§Ã£o (requer que todos os modelos jÃ¡ estejam instalados)
- **Arquivos ignorados**: Por padrÃ£o, o `.gitignore` exclui `*.pdf`, `output/`, `result/`, `.history/` e `models/`

## âš™ï¸ ConfiguraÃ§Ã£o

As configuraÃ§Ãµes do pipeline podem ser ajustadas em `config.py` atravÃ©s da classe `PipelineConfig`:

- **ExtraÃ§Ã£o**: Qualidade mÃ­nima de extraÃ§Ã£o, nÃºmero mÃ­nimo de caracteres Ãºteis
- **Candidatos**: Modelo spaCy, nÃºmero mÃ¡ximo de candidatos por tipo
- **PontuaÃ§Ã£o**: Pesos para diferentes heurÃ­sticas (palavras-chave, frequÃªncia, posiÃ§Ã£o, formato)
- **LLM**: ParÃ¢metros do modelo (contexto, temperatura, tokens mÃ¡ximos)
- **ConfianÃ§a**: Limite mÃ­nimo de confianÃ§a

## ğŸ§ª Testes

O projeto inclui testes para garantir qualidade e determinismo:

```bash
pytest tests/
```

### Testes disponÃ­veis

- `test_determinism.py`: Verifica que os resultados sÃ£o determinÃ­sticos
- `test_extract_quality.py`: Valida a qualidade da extraÃ§Ã£o

## ğŸ“ Notas sobre Determinismo

- O LLM roda com `temperature=0` e `seed` fixo para garantir resultados reproduzÃ­veis
- Ranking e desempates sÃ£o estÃ¡veis (ordem de apariÃ§Ã£o como fallback)
- Em caso de extraÃ§Ã£o ruim, erro de modelo ou erro de validaÃ§Ã£o JSON: retorna `INDEFINIDO`

## ğŸ“ Estrutura do Projeto

```
docling_project/
â”œâ”€â”€ main.py                 # Ponto de entrada principal
â”œâ”€â”€ config.py              # ConfiguraÃ§Ãµes do pipeline
â”œâ”€â”€ requirements.txt       # DependÃªncias do projeto
â”œâ”€â”€ pipeline/              # MÃ³dulos do pipeline
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ blocks.py          # Processamento de blocos de texto
â”‚   â”œâ”€â”€ candidates.py      # GeraÃ§Ã£o de candidatos (NER)
â”‚   â”œâ”€â”€ confidence.py      # CÃ¡lculo de confianÃ§a
â”‚   â”œâ”€â”€ decision_llm.py    # DecisÃ£o final com LLM
â”‚   â”œâ”€â”€ extract_json.py    # ExtraÃ§Ã£o Docling
â”‚   â”œâ”€â”€ scoring.py         # PontuaÃ§Ã£o e ranking
â”‚   â””â”€â”€ utils.py           # UtilitÃ¡rios
â””â”€â”€ tests/                 # Testes automatizados
    â”œâ”€â”€ test_determinism.py
    â””â”€â”€ test_extract_quality.py
```

## ğŸ¤ Contribuindo

Este Ã© um projeto privado. Para contribuiÃ§Ãµes, entre em contato com o mantenedor.


## ğŸ”— ReferÃªncias

- [Docling](https://github.com/DS4SD/docling)
- [spaCy](https://spacy.io/)
- [llama.cpp](https://github.com/ggerganov/llama.cpp)
- [llama-cpp-python](https://github.com/abetlen/llama-cpp-python)
